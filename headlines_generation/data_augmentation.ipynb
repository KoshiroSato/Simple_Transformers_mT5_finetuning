{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fafed647"},"outputs":[],"source":["import random\n","import numpy as np\n","import pandas as pd\n","from time import sleep\n","from textblob import TextBlob\n","from textblob.translate import NotTranslated\n","import torch\n","from nlpaug.augmenter.word import ContextualWordEmbsAug\n","from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2064712c"},"outputs":[],"source":["data = pd.read_csv('../data/headlines_generation_corpus/train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d060427"},"outputs":[],"source":["## Aug1: Sentence Shuffle\n","def apply_sentenceShuffle(df, shuffle=False, random_state=42):\n","\n","    def random_sentence_shuffle(text, random_state=random_state):\n","        '''\n","        文章をスプリットし、無作為にシャッフル\n","        '''\n","        random.seed(random_state)\n","        text = text.split('。')\n","        random.shuffle(text)\n","        text = ' '.join(text)\n","        return text\n","\n","    def sentence_shuffle(text):\n","        '''\n","        example:                           \n","        [sentenceA, sentenceB, sentenceC] --apply--> [sentenceC, sentenceA, sentenceB] \n","        '''\n","        text_arr = np.array(text.split('。')[:-1])\n","        shuffled = np.roll(text_arr, 1).tolist()\n","        shuffled_text = ' '.join(shuffled)\n","        return shuffled_text\n","\n","    if shuffle == True:\n","        df['input_text'] = pd.Series([str(random_sentence_shuffle(value)) for value in df['input_text']])\n","    else:\n","        df['input_text'] = pd.Series([str(sentence_shuffle(value)) for value in df['input_text']])\n","    return df\n","\n","\n","## Aug2: Back Translation\n","def apply_backTranslation(df):\n","\n","    def back_translation(text):\n","        '''\n","        訓練データを\n","        日本語 → 英語 → 日本語の流れで\n","        逆翻訳する\n","        '''\n","        textblob = TextBlob(text)\n","        try:\n","            textblob = textblob.translate(to='en')\n","            sleep(0.4)\n","            textblob = textblob.translate(to='ja')\n","            sleep(0.4)\n","            return textblob\n","        except NotTranslated:\n","            pass\n","\n","    df['input_text'] = pd.Series([str(back_translation(value)) for value in tqdm(df['input_text'])])\n","    return df\n","\n","# not use\n","# ## Aug3: Contextual Word Embedded Augmentation by BERT\n","# def word_embedded_aug(df):\n","#     '''\n","#     BERTによる類似単語埋め込み（置き換え）増強\n","#     '''\n","#     params = {\n","#         'model_path': 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n","#         'aug_p': 0.1,\n","#         'batch_size': 32,\n","#         'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n","#     }\n","#     aug_df = df.copy()\n","#     aug = ContextualWordEmbsAug(**params)\n","#     aug_df['input_text'] = [aug.augment(text) for text in tqdm(df['input_text'])]\n","#     # denoising\n","#     aug_df['input_text'] = aug_df['input_text'].apply(lambda x: x.replace(' ',''))\n","#     aug_df['include_unk'] = aug_df['input_text'].str.contains('[UNK]')\n","#     aug_df = aug_df.query('include_unk == False')\n","#     aug_df = aug_df.drop(['include_unk'], axis=1)\n","#     return aug_df\n","\n","# not use\n","# # Aug4: Random Character Deletion\n","# def random_char_deletion(text, random_state=42):\n","#     '''\n","#     無作為に1センテンスにつき、1文字を削除\n","#     '''\n","#     random.seed(random_state)\n","#     new_text = []\n","#     for sentence in text.split('。')[:-1]:\n","#         sentence += '。'\n","#         chars = list(sentence)\n","#         while True:\n","#             del_char = random.choice(chars)\n","#             if del_char not in ['、', '。']:\n","#                 break\n","#         chars.remove(del_char)\n","#         new_sentence = ''.join(chars)\n","#         new_text.append(new_sentence)\n","#     new_text = ''.join(new_text)\n","#     return new_text\n","\n","# def apply_randomCharDeletion(df):\n","#     new_df = df.copy()\n","#     new_df['input_text'] = df['input_text'].map(random_char_deletion)\n","#     return new_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"115e89b1"},"outputs":[],"source":["ss_aug_data = apply_sentenceShuffle(data, shuffle=False)\n","bt_aug_data = apply_backTranslation(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4444cb5c"},"outputs":[],"source":["ss_aug_data.to_csv('../data/headlines_generation_corpus/sentence_shuffle_aug_data.csv', index=False)\n","bt_aug_data.to_csv('../data/headlines_generation_corpus/back_translation_aug_data.csv', index=False)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"data_augmentation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":5}
